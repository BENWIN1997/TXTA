{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd01cf7",
   "metadata": {},
   "source": [
    "## Benwin - D22032\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73779f1",
   "metadata": {},
   "source": [
    "## Text Classification: Problem 2 (using word2vec embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4bd7d",
   "metadata": {},
   "source": [
    "### Importing the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b8c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,roc_auc_score,accuracy_score\n",
    "from sklearn import tree\n",
    "\n",
    "# For fitting Classification tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.ensemble as ensemble #import ensemble for bagging\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report,recall_score,f1_score #import Scoring metric\n",
    "from sklearn.ensemble import RandomForestClassifier #import randomforest Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier #import adaboostclassifer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c7a48",
   "metadata": {},
   "source": [
    "### Upload and read the Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b966bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a817a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have to say, Apple has by far the best custo...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iOS 7 is so fricking smooth &amp; beautiful!! #Tha...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOVE U @APPLE</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you @apple, loving my new iPhone 5S!!!!!...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@apple has the best customer service. In and ...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Avg\n",
       "0  I have to say, Apple has by far the best custo...  2.0\n",
       "1  iOS 7 is so fricking smooth & beautiful!! #Tha...  2.0\n",
       "2                                      LOVE U @APPLE  1.8\n",
       "3  Thank you @apple, loving my new iPhone 5S!!!!!...  1.8\n",
       "4  .@apple has the best customer service. In and ...  1.8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6378c7",
   "metadata": {},
   "source": [
    "### Test Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7462b6",
   "metadata": {},
   "source": [
    "#### Step 1. Converting the text into lower cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075c739d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ios 7 is so fricking smooth & beautiful!! #thanxapple @apple'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert a string into lower case\n",
    "twt = tweets.Tweet[1]\n",
    "twt.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a128863b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i have to say, apple has by far the best custo...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ios 7 is so fricking smooth &amp; beautiful!! #tha...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love u @apple</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank you @apple, loving my new iphone 5s!!!!!...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@apple has the best customer service. in and ...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Avg\n",
       "0  i have to say, apple has by far the best custo...  2.0\n",
       "1  ios 7 is so fricking smooth & beautiful!! #tha...  2.0\n",
       "2                                      love u @apple  1.8\n",
       "3  thank you @apple, loving my new iphone 5s!!!!!...  1.8\n",
       "4  .@apple has the best customer service. in and ...  1.8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalizing all the tweets\n",
    "for i in range(len(tweets.Tweet)):\n",
    "    tweets.Tweet[i] = tweets.Tweet[i].lower()\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a8a35",
   "metadata": {},
   "source": [
    "#### Step 2. Remove Punctuations/special symbols  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e10d367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iOS 7 is so fricking smooth  beautiful ThanxApple Apple'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing punctuation from a single tweet\n",
    "import string\n",
    "p = string.punctuation\n",
    "remv_punc = str.maketrans(\"\", \"\", p)\n",
    "a=twt.translate(remv_punc)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "506bee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuation from all the tweets\n",
    "for i in range(len(tweets.Tweet)):\n",
    "    tweets.Tweet[i] = tweets.Tweet[i].translate(remv_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "873185e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i have to say apple has by far the best custom...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ios 7 is so fricking smooth  beautiful thanxap...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love u apple</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank you apple loving my new iphone 5s  apple...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple has the best customer service in and out...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>freak apple</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>why cant i freaking see pictures on my tl im a...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>apple you freaking cows freak you</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>apple i hate you why is my phone not working i...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>agounalakis thats nasty apple is a nasty brat</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1181 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet  Avg\n",
       "0     i have to say apple has by far the best custom...  2.0\n",
       "1     ios 7 is so fricking smooth  beautiful thanxap...  2.0\n",
       "2                                          love u apple  1.8\n",
       "3     thank you apple loving my new iphone 5s  apple...  1.8\n",
       "4     apple has the best customer service in and out...  1.8\n",
       "...                                                 ...  ...\n",
       "1176                                        freak apple -2.0\n",
       "1177  why cant i freaking see pictures on my tl im a... -2.0\n",
       "1178                  apple you freaking cows freak you -2.0\n",
       "1179  apple i hate you why is my phone not working i... -2.0\n",
       "1180      agounalakis thats nasty apple is a nasty brat -2.0\n",
       "\n",
       "[1181 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e44e6e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i have to say apple has by far the best custom...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ios 7 is so fricking smooth  beautiful thanxap...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love u apple</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank you apple loving my new iphone 5s  apple...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple has the best customer service in and out...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>freak apple</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>why cant i freaking see pictures on my tl im a...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>apple you freaking cows freak you</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>apple i hate you why is my phone not working i...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>agounalakis thats nasty apple is a nasty brat</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>844 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet  Avg\n",
       "0     i have to say apple has by far the best custom...  2.0\n",
       "1     ios 7 is so fricking smooth  beautiful thanxap...  2.0\n",
       "2                                          love u apple  1.8\n",
       "3     thank you apple loving my new iphone 5s  apple...  1.8\n",
       "4     apple has the best customer service in and out...  1.8\n",
       "...                                                 ...  ...\n",
       "1176                                        freak apple -2.0\n",
       "1177  why cant i freaking see pictures on my tl im a... -2.0\n",
       "1178                  apple you freaking cows freak you -2.0\n",
       "1179  apple i hate you why is my phone not working i... -2.0\n",
       "1180      agounalakis thats nasty apple is a nasty brat -2.0\n",
       "\n",
       "[844 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subsetting the data where average of tweet = 0\n",
    "tweets = tweets.loc[tweets.Avg!=0] \n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f0e2a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i have to say apple has by far the best custom...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ios 7 is so fricking smooth  beautiful thanxap...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love u apple</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank you apple loving my new iphone 5s  apple...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple has the best customer service in and out...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>freak apple</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>why cant i freaking see pictures on my tl im a...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>apple you freaking cows freak you</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>apple i hate you why is my phone not working i...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>agounalakis thats nasty apple is a nasty brat</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>844 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweet  Avg\n",
       "0    i have to say apple has by far the best custom...  2.0\n",
       "1    ios 7 is so fricking smooth  beautiful thanxap...  2.0\n",
       "2                                         love u apple  1.8\n",
       "3    thank you apple loving my new iphone 5s  apple...  1.8\n",
       "4    apple has the best customer service in and out...  1.8\n",
       "..                                                 ...  ...\n",
       "839                                        freak apple -2.0\n",
       "840  why cant i freaking see pictures on my tl im a... -2.0\n",
       "841                  apple you freaking cows freak you -2.0\n",
       "842  apple i hate you why is my phone not working i... -2.0\n",
       "843      agounalakis thats nasty apple is a nasty brat -2.0\n",
       "\n",
       "[844 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Resetting the index of the dataframe and dropping the original index\n",
    "tweets.reset_index(drop=True,inplace=True)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1087d",
   "metadata": {},
   "source": [
    "#### Step 3. Removing Stopwords from all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b70542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36c17561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#NLTK stopword list\n",
    "stop_words = stopwords.words(\"english\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbfaec4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09267aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding the word apple to the list of stopwords\n",
    "stop_words.append(\"apple\")\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26ca0ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'apple']\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae86447b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say far best customer care service ever received appstore'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove stop words from a single tweet\n",
    "\" \".join([w for w in tweets.Tweet[0].split() if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed246ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['say',\n",
       " 'far',\n",
       " 'best',\n",
       " 'customer',\n",
       " 'care',\n",
       " 'service',\n",
       " 'ever',\n",
       " 'received',\n",
       " 'appstore']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in tweets.Tweet[0].split() if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "395d7eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d8bb402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords from all the tweets\n",
    "for i in range(len(tweets.Tweet)):\n",
    "    tweets.Tweet[i] = \" \".join([w for w in tweets.Tweet[i].split() if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e47abf3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say far best customer care service ever receiv...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ios 7 fricking smooth beautiful thanxapple</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love u</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank loving new iphone 5s iphone5s pictwitter...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best customer service new phone 10min</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Avg\n",
       "0  say far best customer care service ever receiv...  2.0\n",
       "1         ios 7 fricking smooth beautiful thanxapple  2.0\n",
       "2                                             love u  1.8\n",
       "3  thank loving new iphone 5s iphone5s pictwitter...  1.8\n",
       "4              best customer service new phone 10min  1.8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787dc7e",
   "metadata": {},
   "source": [
    "#### Step 4 . Remove white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b43d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tweets.Tweet)):\n",
    "    tweets.Tweet[i] = tweets.Tweet[i].replace(\"  \", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14563f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say far best customer care service ever receiv...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ios 7 fricking smooth beautiful thanxapple</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love u</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank loving new iphone 5s iphone5s pictwitter...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best customer service new phone 10min</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>freak</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>cant freaking see pictures tl im annoyed freak...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>freaking cows freak</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>hate phone working im going freak</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>agounalakis thats nasty nasty brat</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>844 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweet  Avg\n",
       "0    say far best customer care service ever receiv...  2.0\n",
       "1           ios 7 fricking smooth beautiful thanxapple  2.0\n",
       "2                                               love u  1.8\n",
       "3    thank loving new iphone 5s iphone5s pictwitter...  1.8\n",
       "4                best customer service new phone 10min  1.8\n",
       "..                                                 ...  ...\n",
       "839                                              freak -2.0\n",
       "840  cant freaking see pictures tl im annoyed freak... -2.0\n",
       "841                                freaking cows freak -2.0\n",
       "842                  hate phone working im going freak -2.0\n",
       "843                 agounalakis thats nasty nasty brat -2.0\n",
       "\n",
       "[844 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cad4e2a8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say far best customer care service ever receiv...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ios fricking smooth beautiful thanxapple</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love u</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank loving new iphone 5s iphone5s pictwitter...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best customer service new phone 10min</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>freak</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>cant freaking see pictures tl im annoyed freak...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>freaking cows freak</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>hate phone working im going freak</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>agounalakis thats nasty nasty brat</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>844 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweet  Avg\n",
       "0    say far best customer care service ever receiv...  2.0\n",
       "1             ios fricking smooth beautiful thanxapple  2.0\n",
       "2                                               love u  1.8\n",
       "3    thank loving new iphone 5s iphone5s pictwitter...  1.8\n",
       "4                best customer service new phone 10min  1.8\n",
       "..                                                 ...  ...\n",
       "839                                              freak -2.0\n",
       "840  cant freaking see pictures tl im annoyed freak... -2.0\n",
       "841                                freaking cows freak -2.0\n",
       "842                  hate phone working im going freak -2.0\n",
       "843                 agounalakis thats nasty nasty brat -2.0\n",
       "\n",
       "[844 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(tweets.Tweet)):\n",
    "    tweets.Tweet[i] = \" \".join([w for w in tweets.Tweet[i].split() if not w.isdigit()])\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f36ad",
   "metadata": {},
   "source": [
    "# Word2vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32b4b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = KeyedVectors.load_word2vec_format('C:\\keniwn\\GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50206c7",
   "metadata": {},
   "source": [
    "### Get word embeddings of the our Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdaea978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>',\n",
       " 'in',\n",
       " 'for',\n",
       " 'that',\n",
       " 'is',\n",
       " 'on',\n",
       " '##',\n",
       " 'The',\n",
       " 'with',\n",
       " 'said',\n",
       " 'was',\n",
       " 'the',\n",
       " 'at',\n",
       " 'not',\n",
       " 'as',\n",
       " 'it',\n",
       " 'be',\n",
       " 'from',\n",
       " 'by',\n",
       " 'are',\n",
       " 'I',\n",
       " 'have',\n",
       " 'he',\n",
       " 'will',\n",
       " 'has',\n",
       " '####',\n",
       " 'his',\n",
       " 'an',\n",
       " 'this',\n",
       " 'or',\n",
       " 'their',\n",
       " 'who',\n",
       " 'they',\n",
       " 'but',\n",
       " '$',\n",
       " 'had',\n",
       " 'year',\n",
       " 'were',\n",
       " 'we',\n",
       " 'more',\n",
       " '###',\n",
       " 'up',\n",
       " 'been',\n",
       " 'you',\n",
       " 'its',\n",
       " 'one',\n",
       " 'about',\n",
       " 'would',\n",
       " 'which',\n",
       " 'out',\n",
       " 'can',\n",
       " 'It',\n",
       " 'all',\n",
       " 'also',\n",
       " 'two',\n",
       " 'after',\n",
       " 'first',\n",
       " 'He',\n",
       " 'do',\n",
       " 'time',\n",
       " 'than',\n",
       " 'when',\n",
       " 'We',\n",
       " 'over',\n",
       " 'last',\n",
       " 'new',\n",
       " 'other',\n",
       " 'her',\n",
       " 'people',\n",
       " 'into',\n",
       " 'In',\n",
       " 'our',\n",
       " 'there',\n",
       " 'A',\n",
       " 'she',\n",
       " 'could',\n",
       " 'just',\n",
       " 'years',\n",
       " 'some',\n",
       " 'U.S.',\n",
       " 'three',\n",
       " 'million',\n",
       " 'them',\n",
       " 'what',\n",
       " 'But',\n",
       " 'so',\n",
       " 'no',\n",
       " 'like',\n",
       " 'if',\n",
       " 'only',\n",
       " 'percent',\n",
       " 'get',\n",
       " 'did',\n",
       " 'him',\n",
       " 'game',\n",
       " 'back',\n",
       " 'because',\n",
       " 'now',\n",
       " '#.#',\n",
       " 'before',\n",
       " 'company',\n",
       " 'any',\n",
       " 'team',\n",
       " 'against',\n",
       " 'off',\n",
       " 'This',\n",
       " 'most',\n",
       " 'made',\n",
       " 'through',\n",
       " 'make',\n",
       " 'second',\n",
       " 'state',\n",
       " 'well',\n",
       " 'day',\n",
       " 'season',\n",
       " 'says',\n",
       " 'week',\n",
       " 'where',\n",
       " 'while',\n",
       " 'down',\n",
       " 'being',\n",
       " 'government',\n",
       " 'your',\n",
       " '#-#',\n",
       " 'home',\n",
       " 'going',\n",
       " 'my',\n",
       " 'good',\n",
       " 'They',\n",
       " \"'re\",\n",
       " 'should',\n",
       " 'many',\n",
       " 'way',\n",
       " 'those',\n",
       " 'four',\n",
       " 'during',\n",
       " 'such',\n",
       " 'may',\n",
       " 'very',\n",
       " 'how',\n",
       " 'since',\n",
       " 'work',\n",
       " 'take',\n",
       " 'including',\n",
       " 'high',\n",
       " 'then',\n",
       " '%',\n",
       " 'next',\n",
       " '#,###',\n",
       " 'By',\n",
       " 'much',\n",
       " 'still',\n",
       " 'go',\n",
       " 'think',\n",
       " 'old',\n",
       " 'even',\n",
       " '#.##',\n",
       " 'world',\n",
       " 'see',\n",
       " 'say',\n",
       " 'business',\n",
       " 'five',\n",
       " 'told',\n",
       " 'under',\n",
       " 'us',\n",
       " '1',\n",
       " 'these',\n",
       " 'If',\n",
       " 'right',\n",
       " 'And',\n",
       " 'me',\n",
       " 'between',\n",
       " 'play',\n",
       " 'help',\n",
       " '##,###',\n",
       " 'market',\n",
       " 'That',\n",
       " 'know',\n",
       " 'end',\n",
       " 'AP',\n",
       " 'long',\n",
       " 'information',\n",
       " 'points',\n",
       " 'does',\n",
       " 'both',\n",
       " 'There',\n",
       " 'part',\n",
       " 'around',\n",
       " 'police',\n",
       " 'want',\n",
       " \"'ve\",\n",
       " 'based',\n",
       " 'For',\n",
       " 'got',\n",
       " 'third',\n",
       " 'school',\n",
       " 'left',\n",
       " 'another',\n",
       " 'country',\n",
       " 'need',\n",
       " '2',\n",
       " 'best',\n",
       " 'win',\n",
       " 'quarter',\n",
       " 'use',\n",
       " 'today',\n",
       " '##.#',\n",
       " 'same',\n",
       " 'public',\n",
       " 'run',\n",
       " 'Friday',\n",
       " 'set',\n",
       " 'month',\n",
       " 'top',\n",
       " 'billion',\n",
       " 'Tuesday',\n",
       " 'come',\n",
       " 'Monday',\n",
       " 'She',\n",
       " 'city',\n",
       " 'place',\n",
       " 'night',\n",
       " 'six',\n",
       " 'each',\n",
       " 'Thursday',\n",
       " '###,###',\n",
       " 'Wednesday',\n",
       " 'here',\n",
       " 'You',\n",
       " 'group',\n",
       " 'really',\n",
       " 'found',\n",
       " 'As',\n",
       " 'used',\n",
       " '3',\n",
       " 'lot',\n",
       " \"'m\",\n",
       " 'money',\n",
       " 'put',\n",
       " 'games',\n",
       " 'support',\n",
       " 'program',\n",
       " 'half',\n",
       " 'report',\n",
       " 'family',\n",
       " 'months',\n",
       " 'number',\n",
       " 'officials',\n",
       " 'am',\n",
       " 'former',\n",
       " 'own',\n",
       " 'man',\n",
       " 'Saturday',\n",
       " 'too',\n",
       " 'better',\n",
       " 'days',\n",
       " 'came',\n",
       " 'lead',\n",
       " 'life',\n",
       " 'American',\n",
       " '##-##',\n",
       " 'show',\n",
       " 'past',\n",
       " 'took',\n",
       " 'added',\n",
       " 'expected',\n",
       " 'called',\n",
       " 'great',\n",
       " 'State',\n",
       " 'services',\n",
       " 'children',\n",
       " 'hit',\n",
       " 'area',\n",
       " 'system',\n",
       " 'every',\n",
       " 'pm',\n",
       " 'big',\n",
       " 'service',\n",
       " 'few',\n",
       " 'per',\n",
       " 'members',\n",
       " 'Sunday',\n",
       " 'early',\n",
       " 'point',\n",
       " 'start',\n",
       " 'companies',\n",
       " 'little',\n",
       " '&',\n",
       " 'case',\n",
       " 'ago',\n",
       " 'local',\n",
       " 'according',\n",
       " 'never',\n",
       " '5',\n",
       " 'without',\n",
       " 'sales',\n",
       " 'until',\n",
       " 'went',\n",
       " 'players',\n",
       " '##th',\n",
       " 'New_York',\n",
       " 'won',\n",
       " 'financial',\n",
       " 'news',\n",
       " '4',\n",
       " 'When',\n",
       " 'share',\n",
       " 'several',\n",
       " 'free',\n",
       " 'away',\n",
       " '##.##',\n",
       " 'already',\n",
       " 'On',\n",
       " 'industry',\n",
       " \"'ll\",\n",
       " 'call',\n",
       " 'With',\n",
       " 'students',\n",
       " 'line',\n",
       " 'available',\n",
       " 'County',\n",
       " 'making',\n",
       " 'held',\n",
       " 'final',\n",
       " '#:##',\n",
       " 'power',\n",
       " 'plan',\n",
       " 'might',\n",
       " 'least',\n",
       " 'look',\n",
       " 'forward',\n",
       " 'give',\n",
       " 'At',\n",
       " 'again',\n",
       " 'later',\n",
       " 'full',\n",
       " 'must',\n",
       " 'things',\n",
       " 'major',\n",
       " 'community',\n",
       " 'announced',\n",
       " 'open',\n",
       " 'record',\n",
       " 'reported',\n",
       " 'court',\n",
       " 'working',\n",
       " 'able',\n",
       " 'something',\n",
       " 'president',\n",
       " 'meeting',\n",
       " 'keep',\n",
       " 'March',\n",
       " 'future',\n",
       " 'far',\n",
       " 'deal',\n",
       " 'City',\n",
       " 'May',\n",
       " 'development',\n",
       " 'University',\n",
       " 'find',\n",
       " 'times',\n",
       " 'After',\n",
       " 'office',\n",
       " 'led',\n",
       " 'among',\n",
       " 'June',\n",
       " 'increase',\n",
       " 'China',\n",
       " 'John',\n",
       " 'whether',\n",
       " 'cost',\n",
       " 'security',\n",
       " 'job',\n",
       " 'less',\n",
       " 'head',\n",
       " 'seven',\n",
       " 'growth',\n",
       " 'lost',\n",
       " 'pay',\n",
       " 'looking',\n",
       " 'provide',\n",
       " '6',\n",
       " 'To',\n",
       " 'plans',\n",
       " 'products',\n",
       " 'car',\n",
       " 'recent',\n",
       " 'hard',\n",
       " 'always',\n",
       " 'include',\n",
       " 'women',\n",
       " 'across',\n",
       " 'tax',\n",
       " 'water',\n",
       " 'April',\n",
       " 'continue',\n",
       " 'important',\n",
       " 'different',\n",
       " 'close',\n",
       " '7',\n",
       " 'One',\n",
       " 'late',\n",
       " 'decision',\n",
       " 'current',\n",
       " 'law',\n",
       " 'within',\n",
       " 'along',\n",
       " 'played',\n",
       " 'move',\n",
       " 'United_States',\n",
       " 'enough',\n",
       " 'become',\n",
       " 'side',\n",
       " 'national',\n",
       " 'Inc.',\n",
       " 'results',\n",
       " 'level',\n",
       " 'loss',\n",
       " 'economic',\n",
       " 'coach',\n",
       " 'near',\n",
       " 'getting',\n",
       " 'price',\n",
       " 'Department',\n",
       " 'event',\n",
       " 'fourth',\n",
       " 'change',\n",
       " 'All',\n",
       " 'small',\n",
       " 'board',\n",
       " 'National',\n",
       " 'So',\n",
       " 'goal',\n",
       " 'taken',\n",
       " 'field',\n",
       " 'prices',\n",
       " 'weeks',\n",
       " 'men',\n",
       " 'asked',\n",
       " 'eight',\n",
       " 'data',\n",
       " 'shot',\n",
       " 'New',\n",
       " 'started',\n",
       " 'July',\n",
       " 'director',\n",
       " 'President',\n",
       " 'party',\n",
       " 'federal',\n",
       " 'done',\n",
       " 'political',\n",
       " 'minutes',\n",
       " 'taking',\n",
       " 'Company',\n",
       " 'technology',\n",
       " 'project',\n",
       " 'center',\n",
       " 'leading',\n",
       " 'issue',\n",
       " 'though',\n",
       " 'having',\n",
       " 'period',\n",
       " 'likely',\n",
       " 'scored',\n",
       " '8',\n",
       " 'strong',\n",
       " 'series',\n",
       " 'military',\n",
       " 'seen',\n",
       " 'trying',\n",
       " 'What',\n",
       " 'coming',\n",
       " 'process',\n",
       " 'building',\n",
       " 'behind',\n",
       " 'performance',\n",
       " 'management',\n",
       " 'Iraq',\n",
       " 'saying',\n",
       " 'earlier',\n",
       " 'believe',\n",
       " 'oil',\n",
       " 'given',\n",
       " 'Police',\n",
       " 'customers',\n",
       " 'due',\n",
       " 'following',\n",
       " 'term',\n",
       " 'others',\n",
       " 'statement',\n",
       " 'international',\n",
       " 'economy',\n",
       " 'health',\n",
       " 'thing',\n",
       " 'Obama',\n",
       " 'return',\n",
       " 'killed',\n",
       " 'Washington',\n",
       " 'further',\n",
       " 'However',\n",
       " 'doing',\n",
       " 'face',\n",
       " 'low',\n",
       " 'higher',\n",
       " 'site',\n",
       " 'once',\n",
       " 'yet',\n",
       " 'hours',\n",
       " 'America',\n",
       " 'control',\n",
       " 'received',\n",
       " 'rate',\n",
       " 'career',\n",
       " 'Bush',\n",
       " 'teams',\n",
       " 'known',\n",
       " 'offer',\n",
       " 'race',\n",
       " 'ever',\n",
       " 'experience',\n",
       " 'playing',\n",
       " 'name',\n",
       " 'possible',\n",
       " 'countries',\n",
       " 'Mr.',\n",
       " 'average',\n",
       " 'together',\n",
       " 'using',\n",
       " '9',\n",
       " 'cut',\n",
       " 'While',\n",
       " 'total',\n",
       " 'round',\n",
       " 'young',\n",
       " 'nearly',\n",
       " 'shares',\n",
       " 'member',\n",
       " 'campaign',\n",
       " 'media',\n",
       " 'needs',\n",
       " 'why',\n",
       " 'house',\n",
       " 'issues',\n",
       " 'costs',\n",
       " 'fire',\n",
       " '##-#',\n",
       " 'victory',\n",
       " 'player',\n",
       " 'began',\n",
       " 'sure',\n",
       " 'story',\n",
       " 'per_cent',\n",
       " 'North',\n",
       " 'His',\n",
       " 'staff',\n",
       " 'order',\n",
       " 'war',\n",
       " 'large',\n",
       " 'interest',\n",
       " 'stock',\n",
       " 'food',\n",
       " 'research',\n",
       " 'key',\n",
       " 'India',\n",
       " 'South',\n",
       " 'morning',\n",
       " 'conference',\n",
       " 'senior',\n",
       " 'global',\n",
       " 'Center',\n",
       " 'death',\n",
       " 'person',\n",
       " 'thought',\n",
       " 'gave',\n",
       " 'feel',\n",
       " 'energy',\n",
       " 'history',\n",
       " 'recently',\n",
       " 'largest',\n",
       " 'No.',\n",
       " 'general',\n",
       " 'official',\n",
       " 'released',\n",
       " 'wanted',\n",
       " 'meet',\n",
       " 'short',\n",
       " 'outside',\n",
       " 'running',\n",
       " 'live',\n",
       " 'ball',\n",
       " 'online',\n",
       " 'real',\n",
       " 'position',\n",
       " 'fact',\n",
       " 'fell',\n",
       " 'nine',\n",
       " 'December',\n",
       " 'front',\n",
       " 'action',\n",
       " 'defense',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'Mr',\n",
       " 'nation',\n",
       " 'needed',\n",
       " 'special',\n",
       " 'January',\n",
       " 'almost',\n",
       " 'chance',\n",
       " \"'d\",\n",
       " 'result',\n",
       " 'West',\n",
       " 'September',\n",
       " 'reports',\n",
       " 'leader',\n",
       " 'investment',\n",
       " 'yesterday',\n",
       " 'Some',\n",
       " 'leaders',\n",
       " 'ahead',\n",
       " 'production',\n",
       " 'comes',\n",
       " 'No',\n",
       " 'runs',\n",
       " 'match',\n",
       " 'role',\n",
       " 'kind',\n",
       " 'try',\n",
       " 'ended',\n",
       " 'risk',\n",
       " 'areas',\n",
       " 'election',\n",
       " 'workers',\n",
       " 'visit',\n",
       " 'bring',\n",
       " 'road',\n",
       " 'music',\n",
       " 'study',\n",
       " 'makes',\n",
       " 'often',\n",
       " 'release',\n",
       " 'woman',\n",
       " 'vote',\n",
       " 'care',\n",
       " 'town',\n",
       " 'clear',\n",
       " 'comment',\n",
       " 'budget',\n",
       " 'potential',\n",
       " 'single',\n",
       " 'markets',\n",
       " 'policy',\n",
       " 'capital',\n",
       " 'saw',\n",
       " 'access',\n",
       " 'weekend',\n",
       " 'operations',\n",
       " 'whose',\n",
       " 'net',\n",
       " 'House',\n",
       " 'hand',\n",
       " 'increased',\n",
       " 'charges',\n",
       " 'winning',\n",
       " 'trade',\n",
       " 'These',\n",
       " 'income',\n",
       " 'value',\n",
       " 'involved',\n",
       " 'Bank',\n",
       " 'November',\n",
       " 'bill',\n",
       " 'compared',\n",
       " 'anything',\n",
       " 'manager',\n",
       " 'Texas',\n",
       " 'property',\n",
       " 'stop',\n",
       " 'annual',\n",
       " 'private',\n",
       " 'contract',\n",
       " 'died',\n",
       " 'Now',\n",
       " 'hope',\n",
       " 'product',\n",
       " 'fans',\n",
       " 'lower',\n",
       " 'demand',\n",
       " 'News',\n",
       " 'David',\n",
       " 'club',\n",
       " 'comments',\n",
       " 'film',\n",
       " 'yards',\n",
       " 'quality',\n",
       " 'currently',\n",
       " 'events',\n",
       " 'addition',\n",
       " 'couple',\n",
       " 'schools',\n",
       " 'attack',\n",
       " 'region',\n",
       " 'latest',\n",
       " 'opportunity',\n",
       " 'worked',\n",
       " 'course',\n",
       " 'bad',\n",
       " 'fall',\n",
       " 'Group',\n",
       " 'October',\n",
       " 'jobs',\n",
       " 'list',\n",
       " 'let',\n",
       " 'however',\n",
       " 'chief',\n",
       " 'summer',\n",
       " 'programs',\n",
       " 'According',\n",
       " 'revenue',\n",
       " 'Our',\n",
       " 'rose',\n",
       " 'previous',\n",
       " 'TV',\n",
       " 'football',\n",
       " 'biggest',\n",
       " 'employees',\n",
       " 'changes',\n",
       " 'residents',\n",
       " 'means',\n",
       " 'agreement',\n",
       " 'includes',\n",
       " 'post',\n",
       " 'Canada',\n",
       " 'probably',\n",
       " 'related',\n",
       " 'training',\n",
       " 'allowed',\n",
       " 'class',\n",
       " 'bit',\n",
       " 'video',\n",
       " 'Michael',\n",
       " 'An',\n",
       " 'sent',\n",
       " 'education',\n",
       " 'states',\n",
       " 'straight',\n",
       " 'love',\n",
       " 'beat',\n",
       " 'hold',\n",
       " 'turn',\n",
       " 'finished',\n",
       " 'network',\n",
       " 'Smith',\n",
       " 'buy',\n",
       " 'foreign',\n",
       " 'especially',\n",
       " 'groups',\n",
       " 'wants',\n",
       " 'title',\n",
       " 'included',\n",
       " 'turned',\n",
       " 'bank',\n",
       " 'Florida',\n",
       " 'efforts',\n",
       " 'personal',\n",
       " 'businesses',\n",
       " 'August',\n",
       " 'California',\n",
       " 'situation',\n",
       " 'district',\n",
       " 'allow',\n",
       " 'helped',\n",
       " 'body',\n",
       " 'nothing',\n",
       " 'soon',\n",
       " 'safety',\n",
       " 'officer',\n",
       " 'cents',\n",
       " 'Europe',\n",
       " 'St.',\n",
       " 'additional',\n",
       " 'spokesman',\n",
       " 'February',\n",
       " 'wife',\n",
       " 'showed',\n",
       " 'leave',\n",
       " 'investors',\n",
       " 'parents',\n",
       " 'medical',\n",
       " 'spending',\n",
       " 'non',\n",
       " 'London',\n",
       " 'Council',\n",
       " 'matter',\n",
       " 'spent',\n",
       " 'child',\n",
       " 'World',\n",
       " 'effort',\n",
       " 'opening',\n",
       " 'either',\n",
       " 'range',\n",
       " 'question',\n",
       " 'European',\n",
       " 'goals',\n",
       " 'administration',\n",
       " 'friends',\n",
       " 'himself',\n",
       " 'shows',\n",
       " 'difficult',\n",
       " 'kids',\n",
       " 'paid',\n",
       " 'create',\n",
       " 'cash',\n",
       " 'age',\n",
       " 'league',\n",
       " 'form',\n",
       " 'impact',\n",
       " 'drive',\n",
       " 'someone',\n",
       " 'became',\n",
       " 'stay',\n",
       " 'fight',\n",
       " 'significant',\n",
       " 'firm',\n",
       " 'Senate',\n",
       " 'hospital',\n",
       " 'charged',\n",
       " 'operating',\n",
       " 'main',\n",
       " 'book',\n",
       " 'success',\n",
       " 'son',\n",
       " 'trading',\n",
       " '###-####',\n",
       " 'focus',\n",
       " 'room',\n",
       " 'continued',\n",
       " 'Congress',\n",
       " 'everything',\n",
       " 'Park',\n",
       " 'agency',\n",
       " 'brought',\n",
       " 'talk',\n",
       " 'break',\n",
       " 'air',\n",
       " 'software',\n",
       " 'decided',\n",
       " 'Do',\n",
       " 'ready',\n",
       " 'arrested',\n",
       " 'track',\n",
       " 'provides',\n",
       " 'mother',\n",
       " 'base',\n",
       " 'trial',\n",
       " 'phone',\n",
       " 'My',\n",
       " 'build',\n",
       " 'conditions',\n",
       " 'rest',\n",
       " 'Johnson',\n",
       " 'terms',\n",
       " 'expect',\n",
       " 'England',\n",
       " 'Israel',\n",
       " 'despite',\n",
       " 'closed',\n",
       " 'starting',\n",
       " 'provided',\n",
       " 'pressure',\n",
       " 'lives',\n",
       " 'step',\n",
       " 'remain',\n",
       " 'similar',\n",
       " 'charge',\n",
       " 'date',\n",
       " 'whole',\n",
       " 'land',\n",
       " 'growing',\n",
       " 'James',\n",
       " 'Internet',\n",
       " 'projects',\n",
       " 'British',\n",
       " 'cases',\n",
       " 'ground',\n",
       " 'legal',\n",
       " 'International',\n",
       " 'agreed',\n",
       " 'tell',\n",
       " 'test',\n",
       " 'everyone',\n",
       " 'pretty',\n",
       " 'authorities',\n",
       " 'Two',\n",
       " 'above',\n",
       " 'moved',\n",
       " 'profit',\n",
       " 'throughout',\n",
       " 'inside',\n",
       " 'ability',\n",
       " 'overall',\n",
       " 'pass',\n",
       " 'officers',\n",
       " 'rather',\n",
       " 'Australia',\n",
       " 'actually',\n",
       " 'county',\n",
       " 'amount',\n",
       " 'scheduled',\n",
       " 'themselves',\n",
       " 'organization',\n",
       " 'giving',\n",
       " 'credit',\n",
       " 'father',\n",
       " 'drug',\n",
       " 'investigation',\n",
       " 'families',\n",
       " 'Republican',\n",
       " 'funds',\n",
       " 'patients',\n",
       " 'takes',\n",
       " 'systems',\n",
       " 'Japan',\n",
       " 'complete',\n",
       " 'sold',\n",
       " 'practice',\n",
       " 'calls',\n",
       " '•',\n",
       " 'UK',\n",
       " 'force',\n",
       " 'student',\n",
       " 'idea',\n",
       " 'reached',\n",
       " 'reason',\n",
       " 'levels',\n",
       " 'space',\n",
       " 'competition',\n",
       " 'forces',\n",
       " 'sector',\n",
       " 'Last',\n",
       " 'tried',\n",
       " 'common',\n",
       " 'homes',\n",
       " 'stage',\n",
       " 'department',\n",
       " 'named',\n",
       " 'earnings',\n",
       " 'offers',\n",
       " 'star',\n",
       " 'certain',\n",
       " 'double',\n",
       " 'longer',\n",
       " 'followed',\n",
       " 'cause',\n",
       " 'Association',\n",
       " 'signed',\n",
       " 'committee',\n",
       " 'hour',\n",
       " 'college',\n",
       " 'Pakistan',\n",
       " 'users',\n",
       " 'Iran',\n",
       " 'sign',\n",
       " 'living',\n",
       " 'failed',\n",
       " 'reach',\n",
       " 'quickly',\n",
       " 'receive',\n",
       " 'debt',\n",
       " 'sale',\n",
       " 'Board',\n",
       " 'Americans',\n",
       " 'Road',\n",
       " 'Brown',\n",
       " 'insurance',\n",
       " '##:##',\n",
       " 'anyone',\n",
       " 'tournament',\n",
       " 'More',\n",
       " 'gas',\n",
       " 'talks',\n",
       " 'serious',\n",
       " 'required',\n",
       " 'sell',\n",
       " 'construction',\n",
       " 'evidence',\n",
       " 'remains',\n",
       " 'black',\n",
       " 'below',\n",
       " 'improve',\n",
       " 'crisis',\n",
       " 'address',\n",
       " 'questions',\n",
       " 'easy',\n",
       " 'begin',\n",
       " 'view',\n",
       " 'School',\n",
       " 'heard',\n",
       " 'executive',\n",
       " 'raised',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b588679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(a):\n",
    "    return(embeddings[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63571f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting each document into a vector\n",
    "dictionary = {}\n",
    "index_list = []\n",
    "for tweet,index in zip(tweets.Tweet,tweets.index):\n",
    "    rev_list = []\n",
    "    for word in tweet.split():\n",
    "        if word in embeddings.index_to_key:\n",
    "            index_list.append(index)\n",
    "            rev_list.append(vec(word))\n",
    "    dictionary[tweet] = np.sum(np.array(rev_list),axis=0)\n",
    "index_set = set(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05b2a36d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>say far best customer care service ever received appstore</th>\n",
       "      <td>-0.493164</td>\n",
       "      <td>-0.095337</td>\n",
       "      <td>0.296143</td>\n",
       "      <td>0.540649</td>\n",
       "      <td>-0.170532</td>\n",
       "      <td>0.215515</td>\n",
       "      <td>0.673096</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>1.091064</td>\n",
       "      <td>0.981018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279327</td>\n",
       "      <td>0.084229</td>\n",
       "      <td>-1.071533</td>\n",
       "      <td>0.579533</td>\n",
       "      <td>0.664551</td>\n",
       "      <td>0.483276</td>\n",
       "      <td>-0.215881</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>0.313599</td>\n",
       "      <td>-0.570312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ios fricking smooth beautiful thanxapple</th>\n",
       "      <td>0.127014</td>\n",
       "      <td>-0.030273</td>\n",
       "      <td>0.273621</td>\n",
       "      <td>-0.040039</td>\n",
       "      <td>-0.403809</td>\n",
       "      <td>-0.032959</td>\n",
       "      <td>0.408875</td>\n",
       "      <td>-0.386108</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.597656</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.307495</td>\n",
       "      <td>0.284668</td>\n",
       "      <td>-0.612305</td>\n",
       "      <td>-0.289963</td>\n",
       "      <td>-0.227783</td>\n",
       "      <td>0.101074</td>\n",
       "      <td>-0.539062</td>\n",
       "      <td>-0.494812</td>\n",
       "      <td>-0.041748</td>\n",
       "      <td>0.389404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love u</th>\n",
       "      <td>-0.150879</td>\n",
       "      <td>-0.105713</td>\n",
       "      <td>0.189941</td>\n",
       "      <td>0.146729</td>\n",
       "      <td>-0.448242</td>\n",
       "      <td>-0.060059</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>-0.544922</td>\n",
       "      <td>-0.199707</td>\n",
       "      <td>0.302246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082275</td>\n",
       "      <td>0.326172</td>\n",
       "      <td>-0.358398</td>\n",
       "      <td>-0.585938</td>\n",
       "      <td>-0.205322</td>\n",
       "      <td>-0.469727</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>-0.369141</td>\n",
       "      <td>-0.176514</td>\n",
       "      <td>0.253418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thank loving new iphone 5s iphone5s pictwittercomxmhjcu4pcb</th>\n",
       "      <td>-0.685974</td>\n",
       "      <td>-0.154663</td>\n",
       "      <td>0.053223</td>\n",
       "      <td>0.654785</td>\n",
       "      <td>-0.353271</td>\n",
       "      <td>-0.230469</td>\n",
       "      <td>0.068115</td>\n",
       "      <td>-0.525146</td>\n",
       "      <td>-0.131836</td>\n",
       "      <td>0.474243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090332</td>\n",
       "      <td>0.766357</td>\n",
       "      <td>-0.982422</td>\n",
       "      <td>-0.664551</td>\n",
       "      <td>-0.309082</td>\n",
       "      <td>-0.283630</td>\n",
       "      <td>-0.598259</td>\n",
       "      <td>0.078613</td>\n",
       "      <td>-0.448792</td>\n",
       "      <td>0.520508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best customer service new phone 10min</th>\n",
       "      <td>-0.210693</td>\n",
       "      <td>0.010620</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.090088</td>\n",
       "      <td>0.067139</td>\n",
       "      <td>-0.029053</td>\n",
       "      <td>-0.178711</td>\n",
       "      <td>0.016846</td>\n",
       "      <td>0.424316</td>\n",
       "      <td>0.662659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328705</td>\n",
       "      <td>0.305664</td>\n",
       "      <td>-0.824219</td>\n",
       "      <td>0.041870</td>\n",
       "      <td>0.265381</td>\n",
       "      <td>0.335083</td>\n",
       "      <td>-0.138672</td>\n",
       "      <td>0.325439</td>\n",
       "      <td>-0.018555</td>\n",
       "      <td>-0.471680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freak u</th>\n",
       "      <td>-0.048828</td>\n",
       "      <td>0.039551</td>\n",
       "      <td>0.169617</td>\n",
       "      <td>0.110596</td>\n",
       "      <td>-0.368652</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>-0.050781</td>\n",
       "      <td>-0.541016</td>\n",
       "      <td>-0.139648</td>\n",
       "      <td>-0.018066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467773</td>\n",
       "      <td>0.381836</td>\n",
       "      <td>-0.319824</td>\n",
       "      <td>-0.239258</td>\n",
       "      <td>-0.363281</td>\n",
       "      <td>-0.510742</td>\n",
       "      <td>-0.338379</td>\n",
       "      <td>-0.149780</td>\n",
       "      <td>-0.075073</td>\n",
       "      <td>0.107422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cant freaking see pictures tl im annoyed freak twitter</th>\n",
       "      <td>0.735931</td>\n",
       "      <td>0.201782</td>\n",
       "      <td>0.265518</td>\n",
       "      <td>1.291504</td>\n",
       "      <td>-1.499268</td>\n",
       "      <td>0.956970</td>\n",
       "      <td>0.374115</td>\n",
       "      <td>-1.322784</td>\n",
       "      <td>0.530029</td>\n",
       "      <td>0.329895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783691</td>\n",
       "      <td>1.047729</td>\n",
       "      <td>-0.649292</td>\n",
       "      <td>-0.248840</td>\n",
       "      <td>-1.650391</td>\n",
       "      <td>-1.156738</td>\n",
       "      <td>-0.439453</td>\n",
       "      <td>-0.505722</td>\n",
       "      <td>-0.418335</td>\n",
       "      <td>0.028564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freaking cows freak</th>\n",
       "      <td>0.586914</td>\n",
       "      <td>-0.016846</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>0.423340</td>\n",
       "      <td>-0.325684</td>\n",
       "      <td>0.740234</td>\n",
       "      <td>0.166382</td>\n",
       "      <td>-0.296600</td>\n",
       "      <td>0.048584</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114258</td>\n",
       "      <td>0.328918</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.109863</td>\n",
       "      <td>-0.231445</td>\n",
       "      <td>-0.142578</td>\n",
       "      <td>-0.358887</td>\n",
       "      <td>0.240845</td>\n",
       "      <td>0.085541</td>\n",
       "      <td>-0.060547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hate phone working im going freak</th>\n",
       "      <td>0.347351</td>\n",
       "      <td>0.309204</td>\n",
       "      <td>0.310913</td>\n",
       "      <td>0.426514</td>\n",
       "      <td>-0.717896</td>\n",
       "      <td>0.939087</td>\n",
       "      <td>0.136658</td>\n",
       "      <td>-0.739258</td>\n",
       "      <td>0.257324</td>\n",
       "      <td>0.149170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155029</td>\n",
       "      <td>0.922363</td>\n",
       "      <td>-0.669189</td>\n",
       "      <td>-0.155579</td>\n",
       "      <td>-0.644287</td>\n",
       "      <td>-0.572754</td>\n",
       "      <td>-0.270996</td>\n",
       "      <td>-0.055420</td>\n",
       "      <td>0.218994</td>\n",
       "      <td>-0.158936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agounalakis thats nasty nasty brat</th>\n",
       "      <td>0.386719</td>\n",
       "      <td>0.052124</td>\n",
       "      <td>0.182861</td>\n",
       "      <td>0.861328</td>\n",
       "      <td>-0.792969</td>\n",
       "      <td>0.715820</td>\n",
       "      <td>-0.126495</td>\n",
       "      <td>-0.231934</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.676025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547852</td>\n",
       "      <td>0.055420</td>\n",
       "      <td>-0.803223</td>\n",
       "      <td>0.238281</td>\n",
       "      <td>-0.302734</td>\n",
       "      <td>-0.732422</td>\n",
       "      <td>0.116943</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.046753</td>\n",
       "      <td>0.413818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>827 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         0         1    \\\n",
       "say far best customer care service ever receive... -0.493164 -0.095337   \n",
       "ios fricking smooth beautiful thanxapple            0.127014 -0.030273   \n",
       "love u                                             -0.150879 -0.105713   \n",
       "thank loving new iphone 5s iphone5s pictwitterc... -0.685974 -0.154663   \n",
       "best customer service new phone 10min              -0.210693  0.010620   \n",
       "...                                                      ...       ...   \n",
       "freak u                                            -0.048828  0.039551   \n",
       "cant freaking see pictures tl im annoyed freak ...  0.735931  0.201782   \n",
       "freaking cows freak                                 0.586914 -0.016846   \n",
       "hate phone working im going freak                   0.347351  0.309204   \n",
       "agounalakis thats nasty nasty brat                  0.386719  0.052124   \n",
       "\n",
       "                                                         2         3    \\\n",
       "say far best customer care service ever receive...  0.296143  0.540649   \n",
       "ios fricking smooth beautiful thanxapple            0.273621 -0.040039   \n",
       "love u                                              0.189941  0.146729   \n",
       "thank loving new iphone 5s iphone5s pictwitterc...  0.053223  0.654785   \n",
       "best customer service new phone 10min               0.281250  0.090088   \n",
       "...                                                      ...       ...   \n",
       "freak u                                             0.169617  0.110596   \n",
       "cant freaking see pictures tl im annoyed freak ...  0.265518  1.291504   \n",
       "freaking cows freak                                 0.001648  0.423340   \n",
       "hate phone working im going freak                   0.310913  0.426514   \n",
       "agounalakis thats nasty nasty brat                  0.182861  0.861328   \n",
       "\n",
       "                                                         4         5    \\\n",
       "say far best customer care service ever receive... -0.170532  0.215515   \n",
       "ios fricking smooth beautiful thanxapple           -0.403809 -0.032959   \n",
       "love u                                             -0.448242 -0.060059   \n",
       "thank loving new iphone 5s iphone5s pictwitterc... -0.353271 -0.230469   \n",
       "best customer service new phone 10min               0.067139 -0.029053   \n",
       "...                                                      ...       ...   \n",
       "freak u                                            -0.368652  0.201172   \n",
       "cant freaking see pictures tl im annoyed freak ... -1.499268  0.956970   \n",
       "freaking cows freak                                -0.325684  0.740234   \n",
       "hate phone working im going freak                  -0.717896  0.939087   \n",
       "agounalakis thats nasty nasty brat                 -0.792969  0.715820   \n",
       "\n",
       "                                                         6         7    \\\n",
       "say far best customer care service ever receive...  0.673096 -0.081055   \n",
       "ios fricking smooth beautiful thanxapple            0.408875 -0.386108   \n",
       "love u                                              0.079102 -0.544922   \n",
       "thank loving new iphone 5s iphone5s pictwitterc...  0.068115 -0.525146   \n",
       "best customer service new phone 10min              -0.178711  0.016846   \n",
       "...                                                      ...       ...   \n",
       "freak u                                            -0.050781 -0.541016   \n",
       "cant freaking see pictures tl im annoyed freak ...  0.374115 -1.322784   \n",
       "freaking cows freak                                 0.166382 -0.296600   \n",
       "hate phone working im going freak                   0.136658 -0.739258   \n",
       "agounalakis thats nasty nasty brat                 -0.126495 -0.231934   \n",
       "\n",
       "                                                         8         9    ...  \\\n",
       "say far best customer care service ever receive...  1.091064  0.981018  ...   \n",
       "ios fricking smooth beautiful thanxapple            0.112793  0.597656  ...   \n",
       "love u                                             -0.199707  0.302246  ...   \n",
       "thank loving new iphone 5s iphone5s pictwitterc... -0.131836  0.474243  ...   \n",
       "best customer service new phone 10min               0.424316  0.662659  ...   \n",
       "...                                                      ...       ...  ...   \n",
       "freak u                                            -0.139648 -0.018066  ...   \n",
       "cant freaking see pictures tl im annoyed freak ...  0.530029  0.329895  ...   \n",
       "freaking cows freak                                 0.048584 -0.116699  ...   \n",
       "hate phone working im going freak                   0.257324  0.149170  ...   \n",
       "agounalakis thats nasty nasty brat                  0.094727  0.676025  ...   \n",
       "\n",
       "                                                         290       291  \\\n",
       "say far best customer care service ever receive... -0.279327  0.084229   \n",
       "ios fricking smooth beautiful thanxapple           -0.307495  0.284668   \n",
       "love u                                              0.082275  0.326172   \n",
       "thank loving new iphone 5s iphone5s pictwitterc...  0.090332  0.766357   \n",
       "best customer service new phone 10min               0.328705  0.305664   \n",
       "...                                                      ...       ...   \n",
       "freak u                                             0.467773  0.381836   \n",
       "cant freaking see pictures tl im annoyed freak ...  0.783691  1.047729   \n",
       "freaking cows freak                                 0.114258  0.328918   \n",
       "hate phone working im going freak                  -0.155029  0.922363   \n",
       "agounalakis thats nasty nasty brat                  0.547852  0.055420   \n",
       "\n",
       "                                                         292       293  \\\n",
       "say far best customer care service ever receive... -1.071533  0.579533   \n",
       "ios fricking smooth beautiful thanxapple           -0.612305 -0.289963   \n",
       "love u                                             -0.358398 -0.585938   \n",
       "thank loving new iphone 5s iphone5s pictwitterc... -0.982422 -0.664551   \n",
       "best customer service new phone 10min              -0.824219  0.041870   \n",
       "...                                                      ...       ...   \n",
       "freak u                                            -0.319824 -0.239258   \n",
       "cant freaking see pictures tl im annoyed freak ... -0.649292 -0.248840   \n",
       "freaking cows freak                                -0.202637  0.109863   \n",
       "hate phone working im going freak                  -0.669189 -0.155579   \n",
       "agounalakis thats nasty nasty brat                 -0.803223  0.238281   \n",
       "\n",
       "                                                         294       295  \\\n",
       "say far best customer care service ever receive...  0.664551  0.483276   \n",
       "ios fricking smooth beautiful thanxapple           -0.227783  0.101074   \n",
       "love u                                             -0.205322 -0.469727   \n",
       "thank loving new iphone 5s iphone5s pictwitterc... -0.309082 -0.283630   \n",
       "best customer service new phone 10min               0.265381  0.335083   \n",
       "...                                                      ...       ...   \n",
       "freak u                                            -0.363281 -0.510742   \n",
       "cant freaking see pictures tl im annoyed freak ... -1.650391 -1.156738   \n",
       "freaking cows freak                                -0.231445 -0.142578   \n",
       "hate phone working im going freak                  -0.644287 -0.572754   \n",
       "agounalakis thats nasty nasty brat                 -0.302734 -0.732422   \n",
       "\n",
       "                                                         296       297  \\\n",
       "say far best customer care service ever receive... -0.215881 -0.071289   \n",
       "ios fricking smooth beautiful thanxapple           -0.539062 -0.494812   \n",
       "love u                                             -0.130859 -0.369141   \n",
       "thank loving new iphone 5s iphone5s pictwitterc... -0.598259  0.078613   \n",
       "best customer service new phone 10min              -0.138672  0.325439   \n",
       "...                                                      ...       ...   \n",
       "freak u                                            -0.338379 -0.149780   \n",
       "cant freaking see pictures tl im annoyed freak ... -0.439453 -0.505722   \n",
       "freaking cows freak                                -0.358887  0.240845   \n",
       "hate phone working im going freak                  -0.270996 -0.055420   \n",
       "agounalakis thats nasty nasty brat                  0.116943 -0.128662   \n",
       "\n",
       "                                                         298       299  \n",
       "say far best customer care service ever receive...  0.313599 -0.570312  \n",
       "ios fricking smooth beautiful thanxapple           -0.041748  0.389404  \n",
       "love u                                             -0.176514  0.253418  \n",
       "thank loving new iphone 5s iphone5s pictwitterc... -0.448792  0.520508  \n",
       "best customer service new phone 10min              -0.018555 -0.471680  \n",
       "...                                                      ...       ...  \n",
       "freak u                                            -0.075073  0.107422  \n",
       "cant freaking see pictures tl im annoyed freak ... -0.418335  0.028564  \n",
       "freaking cows freak                                 0.085541 -0.060547  \n",
       "hate phone working im going freak                   0.218994 -0.158936  \n",
       "agounalakis thats nasty nasty brat                 -0.046753  0.413818  \n",
       "\n",
       "[827 rows x 300 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_matrix = pd.DataFrame(dictionary).T\n",
    "document_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa6520b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "827"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are duplicate reviews in my dataset\n",
    "tweets.Tweet.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef7779ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>say far best customer care service ever receiv...</td>\n",
       "      <td>-0.493164</td>\n",
       "      <td>-0.095337</td>\n",
       "      <td>0.296143</td>\n",
       "      <td>0.540649</td>\n",
       "      <td>-0.170532</td>\n",
       "      <td>0.215515</td>\n",
       "      <td>0.673096</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084229</td>\n",
       "      <td>-1.071533</td>\n",
       "      <td>0.579533</td>\n",
       "      <td>0.664551</td>\n",
       "      <td>0.483276</td>\n",
       "      <td>-0.215881</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>0.313599</td>\n",
       "      <td>-0.570312</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ios fricking smooth beautiful thanxapple</td>\n",
       "      <td>0.127014</td>\n",
       "      <td>-0.030273</td>\n",
       "      <td>0.273621</td>\n",
       "      <td>-0.040039</td>\n",
       "      <td>-0.403809</td>\n",
       "      <td>-0.032959</td>\n",
       "      <td>0.408875</td>\n",
       "      <td>-0.386108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284668</td>\n",
       "      <td>-0.612305</td>\n",
       "      <td>-0.289963</td>\n",
       "      <td>-0.227783</td>\n",
       "      <td>0.101074</td>\n",
       "      <td>-0.539062</td>\n",
       "      <td>-0.494812</td>\n",
       "      <td>-0.041748</td>\n",
       "      <td>0.389404</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>love u</td>\n",
       "      <td>-0.150879</td>\n",
       "      <td>-0.105713</td>\n",
       "      <td>0.189941</td>\n",
       "      <td>0.146729</td>\n",
       "      <td>-0.448242</td>\n",
       "      <td>-0.060059</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>-0.544922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326172</td>\n",
       "      <td>-0.358398</td>\n",
       "      <td>-0.585938</td>\n",
       "      <td>-0.205322</td>\n",
       "      <td>-0.469727</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>-0.369141</td>\n",
       "      <td>-0.176514</td>\n",
       "      <td>0.253418</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>thank loving new iphone 5s iphone5s pictwitter...</td>\n",
       "      <td>-0.685974</td>\n",
       "      <td>-0.154663</td>\n",
       "      <td>0.053223</td>\n",
       "      <td>0.654785</td>\n",
       "      <td>-0.353271</td>\n",
       "      <td>-0.230469</td>\n",
       "      <td>0.068115</td>\n",
       "      <td>-0.525146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766357</td>\n",
       "      <td>-0.982422</td>\n",
       "      <td>-0.664551</td>\n",
       "      <td>-0.309082</td>\n",
       "      <td>-0.283630</td>\n",
       "      <td>-0.598259</td>\n",
       "      <td>0.078613</td>\n",
       "      <td>-0.448792</td>\n",
       "      <td>0.520508</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>best customer service new phone 10min</td>\n",
       "      <td>-0.210693</td>\n",
       "      <td>0.010620</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.090088</td>\n",
       "      <td>0.067139</td>\n",
       "      <td>-0.029053</td>\n",
       "      <td>-0.178711</td>\n",
       "      <td>0.016846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305664</td>\n",
       "      <td>-0.824219</td>\n",
       "      <td>0.041870</td>\n",
       "      <td>0.265381</td>\n",
       "      <td>0.335083</td>\n",
       "      <td>-0.138672</td>\n",
       "      <td>0.325439</td>\n",
       "      <td>-0.018555</td>\n",
       "      <td>-0.471680</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>822</td>\n",
       "      <td>freak u</td>\n",
       "      <td>-0.048828</td>\n",
       "      <td>0.039551</td>\n",
       "      <td>0.169617</td>\n",
       "      <td>0.110596</td>\n",
       "      <td>-0.368652</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>-0.050781</td>\n",
       "      <td>-0.541016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381836</td>\n",
       "      <td>-0.319824</td>\n",
       "      <td>-0.239258</td>\n",
       "      <td>-0.363281</td>\n",
       "      <td>-0.510742</td>\n",
       "      <td>-0.338379</td>\n",
       "      <td>-0.149780</td>\n",
       "      <td>-0.075073</td>\n",
       "      <td>0.107422</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>823</td>\n",
       "      <td>cant freaking see pictures tl im annoyed freak...</td>\n",
       "      <td>0.735931</td>\n",
       "      <td>0.201782</td>\n",
       "      <td>0.265518</td>\n",
       "      <td>1.291504</td>\n",
       "      <td>-1.499268</td>\n",
       "      <td>0.956970</td>\n",
       "      <td>0.374115</td>\n",
       "      <td>-1.322784</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047729</td>\n",
       "      <td>-0.649292</td>\n",
       "      <td>-0.248840</td>\n",
       "      <td>-1.650391</td>\n",
       "      <td>-1.156738</td>\n",
       "      <td>-0.439453</td>\n",
       "      <td>-0.505722</td>\n",
       "      <td>-0.418335</td>\n",
       "      <td>0.028564</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>824</td>\n",
       "      <td>freaking cows freak</td>\n",
       "      <td>0.586914</td>\n",
       "      <td>-0.016846</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>0.423340</td>\n",
       "      <td>-0.325684</td>\n",
       "      <td>0.740234</td>\n",
       "      <td>0.166382</td>\n",
       "      <td>-0.296600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328918</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.109863</td>\n",
       "      <td>-0.231445</td>\n",
       "      <td>-0.142578</td>\n",
       "      <td>-0.358887</td>\n",
       "      <td>0.240845</td>\n",
       "      <td>0.085541</td>\n",
       "      <td>-0.060547</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>825</td>\n",
       "      <td>hate phone working im going freak</td>\n",
       "      <td>0.347351</td>\n",
       "      <td>0.309204</td>\n",
       "      <td>0.310913</td>\n",
       "      <td>0.426514</td>\n",
       "      <td>-0.717896</td>\n",
       "      <td>0.939087</td>\n",
       "      <td>0.136658</td>\n",
       "      <td>-0.739258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922363</td>\n",
       "      <td>-0.669189</td>\n",
       "      <td>-0.155579</td>\n",
       "      <td>-0.644287</td>\n",
       "      <td>-0.572754</td>\n",
       "      <td>-0.270996</td>\n",
       "      <td>-0.055420</td>\n",
       "      <td>0.218994</td>\n",
       "      <td>-0.158936</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>826</td>\n",
       "      <td>agounalakis thats nasty nasty brat</td>\n",
       "      <td>0.386719</td>\n",
       "      <td>0.052124</td>\n",
       "      <td>0.182861</td>\n",
       "      <td>0.861328</td>\n",
       "      <td>-0.792969</td>\n",
       "      <td>0.715820</td>\n",
       "      <td>-0.126495</td>\n",
       "      <td>-0.231934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055420</td>\n",
       "      <td>-0.803223</td>\n",
       "      <td>0.238281</td>\n",
       "      <td>-0.302734</td>\n",
       "      <td>-0.732422</td>\n",
       "      <td>0.116943</td>\n",
       "      <td>-0.128662</td>\n",
       "      <td>-0.046753</td>\n",
       "      <td>0.413818</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>827 rows × 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     level_0                                              index         0  \\\n",
       "0          0  say far best customer care service ever receiv... -0.493164   \n",
       "1          1           ios fricking smooth beautiful thanxapple  0.127014   \n",
       "2          2                                             love u -0.150879   \n",
       "3          3  thank loving new iphone 5s iphone5s pictwitter... -0.685974   \n",
       "4          4              best customer service new phone 10min -0.210693   \n",
       "..       ...                                                ...       ...   \n",
       "822      822                                            freak u -0.048828   \n",
       "823      823  cant freaking see pictures tl im annoyed freak...  0.735931   \n",
       "824      824                                freaking cows freak  0.586914   \n",
       "825      825                  hate phone working im going freak  0.347351   \n",
       "826      826                 agounalakis thats nasty nasty brat  0.386719   \n",
       "\n",
       "            1         2         3         4         5         6         7  \\\n",
       "0   -0.095337  0.296143  0.540649 -0.170532  0.215515  0.673096 -0.081055   \n",
       "1   -0.030273  0.273621 -0.040039 -0.403809 -0.032959  0.408875 -0.386108   \n",
       "2   -0.105713  0.189941  0.146729 -0.448242 -0.060059  0.079102 -0.544922   \n",
       "3   -0.154663  0.053223  0.654785 -0.353271 -0.230469  0.068115 -0.525146   \n",
       "4    0.010620  0.281250  0.090088  0.067139 -0.029053 -0.178711  0.016846   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "822  0.039551  0.169617  0.110596 -0.368652  0.201172 -0.050781 -0.541016   \n",
       "823  0.201782  0.265518  1.291504 -1.499268  0.956970  0.374115 -1.322784   \n",
       "824 -0.016846  0.001648  0.423340 -0.325684  0.740234  0.166382 -0.296600   \n",
       "825  0.309204  0.310913  0.426514 -0.717896  0.939087  0.136658 -0.739258   \n",
       "826  0.052124  0.182861  0.861328 -0.792969  0.715820 -0.126495 -0.231934   \n",
       "\n",
       "     ...       291       292       293       294       295       296  \\\n",
       "0    ...  0.084229 -1.071533  0.579533  0.664551  0.483276 -0.215881   \n",
       "1    ...  0.284668 -0.612305 -0.289963 -0.227783  0.101074 -0.539062   \n",
       "2    ...  0.326172 -0.358398 -0.585938 -0.205322 -0.469727 -0.130859   \n",
       "3    ...  0.766357 -0.982422 -0.664551 -0.309082 -0.283630 -0.598259   \n",
       "4    ...  0.305664 -0.824219  0.041870  0.265381  0.335083 -0.138672   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "822  ...  0.381836 -0.319824 -0.239258 -0.363281 -0.510742 -0.338379   \n",
       "823  ...  1.047729 -0.649292 -0.248840 -1.650391 -1.156738 -0.439453   \n",
       "824  ...  0.328918 -0.202637  0.109863 -0.231445 -0.142578 -0.358887   \n",
       "825  ...  0.922363 -0.669189 -0.155579 -0.644287 -0.572754 -0.270996   \n",
       "826  ...  0.055420 -0.803223  0.238281 -0.302734 -0.732422  0.116943   \n",
       "\n",
       "          297       298       299  Avg  \n",
       "0   -0.071289  0.313599 -0.570312  NaN  \n",
       "1   -0.494812 -0.041748  0.389404  NaN  \n",
       "2   -0.369141 -0.176514  0.253418  NaN  \n",
       "3    0.078613 -0.448792  0.520508  NaN  \n",
       "4    0.325439 -0.018555 -0.471680  NaN  \n",
       "..        ...       ...       ...  ...  \n",
       "822 -0.149780 -0.075073  0.107422  NaN  \n",
       "823 -0.505722 -0.418335  0.028564  NaN  \n",
       "824  0.240845  0.085541 -0.060547  NaN  \n",
       "825 -0.055420  0.218994 -0.158936  NaN  \n",
       "826 -0.128662 -0.046753  0.413818  NaN  \n",
       "\n",
       "[827 rows x 303 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_matrix=document_matrix.reset_index()\n",
    "document_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2680d051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     NaN\n",
       "1     NaN\n",
       "2     NaN\n",
       "3     NaN\n",
       "4     NaN\n",
       "       ..\n",
       "822   NaN\n",
       "823   NaN\n",
       "824   NaN\n",
       "825   NaN\n",
       "826   NaN\n",
       "Name: Avg, Length: 827, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_matrix['Avg']=np.nan\n",
    "document_matrix['Avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab7b8240",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tweets.Tweet)):\n",
    "    for j in range(len(document_matrix)):\n",
    "        if document_matrix['index'][j]==tweets['Tweet'][i]:\n",
    "            document_matrix['Avg'][j]=tweets['Avg'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc264f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "say far best customer care service ever received appstore      2.0\n",
       "ios fricking smooth beautiful thanxapple                       2.0\n",
       "love u                                                         1.8\n",
       "thank loving new iphone 5s iphone5s pictwittercomxmhjcu4pcb    1.8\n",
       "best customer service new phone 10min                          1.8\n",
       "                                                              ... \n",
       "freak u                                                       -2.0\n",
       "cant freaking see pictures tl im annoyed freak twitter        -2.0\n",
       "freaking cows freak                                           -2.0\n",
       "hate phone working im going freak                             -2.0\n",
       "agounalakis thats nasty nasty brat                            -2.0\n",
       "Name: Avg, Length: 827, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_matrix.set_index('index',inplace=True)\n",
    "y = document_matrix.Avg\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9882d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = document_matrix.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7cd809",
   "metadata": {},
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d04101a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(827, 302)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00d06b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "say far best customer care service ever received appstore      2.0\n",
       "ios fricking smooth beautiful thanxapple                       2.0\n",
       "love u                                                         1.8\n",
       "thank loving new iphone 5s iphone5s pictwittercomxmhjcu4pcb    1.8\n",
       "best customer service new phone 10min                          1.8\n",
       "                                                              ... \n",
       "freak u                                                       -2.0\n",
       "cant freaking see pictures tl im annoyed freak twitter        -2.0\n",
       "freaking cows freak                                           -2.0\n",
       "hate phone working im going freak                             -2.0\n",
       "agounalakis thats nasty nasty brat                            -2.0\n",
       "Name: Avg, Length: 827, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg=document_matrix.Avg[document_matrix.Avg!=0]\n",
    "avg.value_counts()\n",
    "avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5ded1a",
   "metadata": {},
   "source": [
    "### Creating Target Variable y as binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1bbba17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "say far best customer care service ever received appstore      False\n",
       "ios fricking smooth beautiful thanxapple                       False\n",
       "love u                                                         False\n",
       "thank loving new iphone 5s iphone5s pictwittercomxmhjcu4pcb    False\n",
       "best customer service new phone 10min                          False\n",
       "                                                               ...  \n",
       "freak u                                                         True\n",
       "cant freaking see pictures tl im annoyed freak twitter          True\n",
       "freaking cows freak                                             True\n",
       "hate phone working im going freak                               True\n",
       "agounalakis thats nasty nasty brat                              True\n",
       "Name: Avg, Length: 827, dtype: bool"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a target variable\n",
    "y = avg <= 0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aacaca3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index      0\n",
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "        ... \n",
       "296        0\n",
       "297        0\n",
       "298        0\n",
       "299        0\n",
       "Avg      827\n",
       "Length: 302, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a31b6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "da100d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V = W2V.drop([\"Avg\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf09a7e8",
   "metadata": {},
   "source": [
    "# Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "265a5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For train-test split\n",
    "from sklearn.model_selection import train_test_split   #import train_test_split function\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(W2V, y, test_size = 0.20)  # X-predictors, y-target\n",
    "\n",
    "#Train-Validation Split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335d12e",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "13a7bd11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'smh rt cemorecake718 rt swaggal worst rt matiucurvegawd yooo iphone battery sus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Training the model with best hyperparameters\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dt \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[0;32m      6\u001b[0m pred \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\tree\\_classes.py:172\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    170\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 172\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m    176\u001b[0m     X\u001b[38;5;241m.\u001b[39msort_indices()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:591\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[0;32m    590\u001b[0m     check_X_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params}\n\u001b[1;32m--> 591\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[0;32m    593\u001b[0m     check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'smh rt cemorecake718 rt swaggal worst rt matiucurvegawd yooo iphone battery sus'"
     ]
    }
   ],
   "source": [
    "#Training the model with best hyperparameters\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "dt.fit(X_train,y_train)\n",
    "pred = dt.predict(X_valid)\n",
    "\n",
    "pred = dt.predict(X_valid)\n",
    "#y_pred #predictions done by decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy_score of Decision Tree:\",accuracy_score(y_valid,pred)) # accuracy score of Decision tree\n",
    "print(\"F1_score of Decision Tree:\",f1_score(y_valid,pred)) # f1 score of Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b7525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting a decision tree\n",
    "from sklearn import tree\n",
    "\n",
    "fig, axes = plt.subplots(figsize =(3,3), dpi=300)\n",
    "\n",
    "tree.plot_tree(dt, feature_names = df.columns, filled = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae2ba9f",
   "metadata": {},
   "source": [
    "## Fully Growned Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By default, the Decision Tree function doesn’t perform any pruning and allows the tree to grow as much as it can.\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train,y_train)\n",
    "pred = dt.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix \n",
    "cm = confusion_matrix(y_valid,pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy_score of fully growned Decision Tree:\",accuracy_score(y_valid,pred)) # accuracy score of fully growned Decision tree\n",
    "print(\"F1_score of Decision Tree:\",f1_score(y_valid,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e95bb63",
   "metadata": {},
   "source": [
    "## Pruned Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4b624795",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'smh rt cemorecake718 rt swaggal worst rt matiucurvegawd yooo iphone battery sus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#pruning-Finding alpha\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcost_complexity_pruning_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m ccp_alphas \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mccp_alphas\n\u001b[0;32m      4\u001b[0m ccp_alphas\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\tree\\_classes.py:643\u001b[0m, in \u001b[0;36mBaseDecisionTree.cost_complexity_pruning_path\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;124;03m\"\"\"Compute the pruning path during Minimal Cost-Complexity Pruning.\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \n\u001b[0;32m    610\u001b[0m \u001b[38;5;124;03mSee :ref:`minimal_cost_complexity_pruning` for details on the pruning\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m        corresponding alpha value in ``ccp_alphas``.\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    642\u001b[0m est \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mset_params(ccp_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m--> 643\u001b[0m \u001b[43mest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Bunch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mccp_pruning_path(est\u001b[38;5;241m.\u001b[39mtree_))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\tree\\_classes.py:172\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    170\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 172\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m    176\u001b[0m     X\u001b[38;5;241m.\u001b[39msort_indices()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:591\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[0;32m    590\u001b[0m     check_X_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params}\n\u001b[1;32m--> 591\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[0;32m    593\u001b[0m     check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'smh rt cemorecake718 rt swaggal worst rt matiucurvegawd yooo iphone battery sus'"
     ]
    }
   ],
   "source": [
    "#pruning-Finding alpha\n",
    "path = dt.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "ccp_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cae8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = {}         #Dictionary to store alpha and mae\n",
    "validation_accuracy = {}\n",
    "\n",
    "for i in ccp_alphas:\n",
    "    dt = DecisionTreeClassifier(ccp_alpha=i)\n",
    "    dt.fit(X_train,y_train)\n",
    "    pred_valid = dt.predict(X_valid)\n",
    "    pred_train = dt.predict(X_train)\n",
    "\n",
    "    train_accuracy[i] = accuracy_score(y_train,pred_train)\n",
    "    validation_accuracy[i] = accuracy_score(y_valid,pred_valid)\n",
    "\n",
    "opt_ccp = max(validation_accuracy,key = validation_accuracy.get) #Optimum Cost Complexity Parameter\n",
    "opt_ccp #optimal CCP_alphas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prun_dt = DecisionTreeClassifier(ccp_alpha=opt_ccp) #Fitting pruned decision tree model using optimum alpha\n",
    "prun_dt.fit(X_train,y_train)\n",
    "\n",
    "pred = prun_dt.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "85311b14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy_score of Pruned Decision Tree:\u001b[39m\u001b[38;5;124m\"\u001b[39m,accuracy_score(y_valid,\u001b[43mpred\u001b[49m)) \u001b[38;5;66;03m# accuracy score of fully growned Decision tree\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1_score of Decision Tree:\u001b[39m\u001b[38;5;124m\"\u001b[39m,f1_score(y_valid,pred))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy_score of Pruned Decision Tree:\",accuracy_score(y_valid,pred)) # accuracy score of fully growned Decision tree\n",
    "print(\"F1_score of Decision Tree:\",f1_score(y_valid,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3201762",
   "metadata": {},
   "source": [
    "## Bagged Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ebbfd7cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'smh rt cemorecake718 rt swaggal worst rt matiucurvegawd yooo iphone battery sus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [85]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m dt \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier()      \u001b[38;5;66;03m#decision tree model as base classifier for bagging -ensemble\u001b[39;00m\n\u001b[0;32m      3\u001b[0m baggedModel \u001b[38;5;241m=\u001b[39m ensemble\u001b[38;5;241m.\u001b[39mBaggingClassifier(base_estimator\u001b[38;5;241m=\u001b[39mdt) \u001b[38;5;66;03m#bagging with DT models\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mbaggedModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#fitting training data in bagged model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m pred_bg \u001b[38;5;241m=\u001b[39m baggedModel\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_bagging.py:297\u001b[0m, in \u001b[0;36mBaseBagging.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# Convert data (X is required to be 2d and indexable)\u001b[39;00m\n\u001b[0;32m    289\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    290\u001b[0m     X,\n\u001b[0;32m    291\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m )\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_bagging.py:434\u001b[0m, in \u001b[0;36mBaseBagging._fit\u001b[1;34m(self, X, y, max_samples, max_depth, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    431\u001b[0m seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39mn_more_estimators)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds \u001b[38;5;241m=\u001b[39m seeds\n\u001b[1;32m--> 434\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_estimators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_n_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# Reduce\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    453\u001b[0m     itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_results)\n\u001b[0;32m    454\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_bagging.py:138\u001b[0m, in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose, check_input)\u001b[0m\n\u001b[0;32m    135\u001b[0m         not_indices_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mindices_to_mask(indices, n_samples)\n\u001b[0;32m    136\u001b[0m         curr_sample_weight[not_indices_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 138\u001b[0m     \u001b[43mestimator_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     estimator_fit(X[indices][:, features], y[indices])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\tree\\_classes.py:172\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    170\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 172\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m    176\u001b[0m     X\u001b[38;5;241m.\u001b[39msort_indices()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:591\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[0;32m    590\u001b[0m     check_X_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params}\n\u001b[1;32m--> 591\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[0;32m    593\u001b[0m     check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'smh rt cemorecake718 rt swaggal worst rt matiucurvegawd yooo iphone battery sus'"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()      #decision tree model as base classifier for bagging -ensemble\n",
    "\n",
    "baggedModel = ensemble.BaggingClassifier(base_estimator=dt) #bagging with DT models\n",
    "\n",
    "baggedModel.fit(X_train,y_train) #fitting training data in bagged model\n",
    "pred_bg = baggedModel.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aefcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix \n",
    "cm = confusion_matrix(y_valid,pred_bg)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy_score of Bagged Decision Tree:\",accuracy_score(y_valid,pred_bg)) #accuracy of bagged decision tree model\n",
    "print(\"F1_score of Decision Tree:\",f1_score(y_valid,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14362a",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)        # fitting training data in random forest model\n",
    "\n",
    "pred_rf= rf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e39e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix \n",
    "cm = confusion_matrix(y_valid,pred_rf)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "52564733",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [86]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy_score of Random Forest:\u001b[39m\u001b[38;5;124m\"\u001b[39m,accuracy_score(y_valid,\u001b[43mpred_rf\u001b[49m)) \u001b[38;5;66;03m#accuracy of RandomForest model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1_score of Decision Tree:\u001b[39m\u001b[38;5;124m\"\u001b[39m,f1_score(y_valid,pred))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred_rf' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy_score of Random Forest:\",accuracy_score(y_valid,pred_rf)) #accuracy of RandomForest model\n",
    "print(\"F1_score of Decision Tree:\",f1_score(y_valid,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d750bb",
   "metadata": {},
   "source": [
    "## Adaboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4422bf0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'smh rt cemorecake718 rt swaggal worst rt matiucurvegawd yooo iphone battery sus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [87]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m ada_boost \u001b[38;5;241m=\u001b[39m AdaBoostClassifier()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mada_boost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# fitting training data in adaboost model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m pred_ada \u001b[38;5;241m=\u001b[39m ada_boost\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:506\u001b[0m, in \u001b[0;36mAdaBoostClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgorithm must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMME\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMME.R\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# Fit\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:160\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    156\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iboost \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators):\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Boosting step\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m     sample_weight, estimator_weight, estimator_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Early termination\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:568\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;124;03m\"\"\"Implement a single boost.\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \n\u001b[0;32m    531\u001b[0m \u001b[38;5;124;03mPerform a single boost according to the real multi-class SAMME.R\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03m    If None then boosting has terminated early.\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAMME.R\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost_real\u001b[49m\u001b[43m(\u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# elif self.algorithm == \"SAMME\":\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_boost_discrete(iboost, X, y, sample_weight, random_state)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:577\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;124;03m\"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\u001b[39;00m\n\u001b[0;32m    575\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m--> 577\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m y_predict_proba \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iboost \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\tree\\_classes.py:172\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    170\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 172\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m    176\u001b[0m     X\u001b[38;5;241m.\u001b[39msort_indices()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:591\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[0;32m    590\u001b[0m     check_X_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params}\n\u001b[1;32m--> 591\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[0;32m    593\u001b[0m     check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'smh rt cemorecake718 rt swaggal worst rt matiucurvegawd yooo iphone battery sus'"
     ]
    }
   ],
   "source": [
    "ada_boost = AdaBoostClassifier()\n",
    "ada_boost.fit(X_train, y_train) # fitting training data in adaboost model\n",
    "\n",
    "pred_ada = ada_boost.predict(X_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e309afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix \n",
    "cm = confusion_matrix(y_valid,pred_ada)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00454a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy_score of Adaboost:\",accuracy_score(y_valid,pred_ada)) #accuracy of adaboost model\n",
    "print(\"F1_score of Decision Tree:\",f1_score(y_valid,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "327fa369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Recall_score of Adaboost:\",recall_score(y_valid,pred_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2f28bdf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decision tree with depth =5</th>\n",
       "      <th>Fully Growned DT</th>\n",
       "      <th>Pruned DT</th>\n",
       "      <th>Bagged DT</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Adaboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Decision tree with depth =5  Fully Growned DT  Pruned DT  Bagged DT  \\\n",
       "Accuracy                         0.62              0.51       0.66       0.65   \n",
       "F1_score                         0.69              0.60       0.79       0.79   \n",
       "\n",
       "          Random Forest  Adaboost  \n",
       "Accuracy           0.72      0.66  \n",
       "F1_score           0.79      0.79  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Decision tree with depth =5': [0.62,0.69], 'Fully Growned DT': [0.51,0.60], 'Pruned DT': [0.66,0.79], 'Bagged DT': [0.65,0.79], 'Random Forest': [0.72,0.79], 'Adaboost': [0.66,0.79] }, index=['Accuracy','F1_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9e304",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ae881",
   "metadata": {},
   "source": [
    "# 2. TF-IDF Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c3733",
   "metadata": {},
   "source": [
    "### Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9895694",
   "metadata": {},
   "source": [
    "#### 1. Get the Bag-Of-Words (BOW) Dataframe with TF-IDF vectorizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fc46abdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say far best customer care service ever receiv...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ios fricking smooth beautiful thanxapple</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love u</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank loving new iphone 5s iphone5s pictwitter...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best customer service new phone 10min</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>freak</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>cant freaking see pictures tl im annoyed freak...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>freaking cows freak</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>hate phone working im going freak</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>agounalakis thats nasty nasty brat</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>844 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweet  Avg\n",
       "0    say far best customer care service ever receiv...  2.0\n",
       "1             ios fricking smooth beautiful thanxapple  2.0\n",
       "2                                               love u  1.8\n",
       "3    thank loving new iphone 5s iphone5s pictwitter...  1.8\n",
       "4                best customer service new phone 10min  1.8\n",
       "..                                                 ...  ...\n",
       "839                                              freak -2.0\n",
       "840  cant freaking see pictures tl im annoyed freak... -2.0\n",
       "841                                freaking cows freak -2.0\n",
       "842                  hate phone working im going freak -2.0\n",
       "843                 agounalakis thats nasty nasty brat -2.0\n",
       "\n",
       "[844 rows x 2 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1d8cf70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10min</th>\n",
       "      <th>13apple</th>\n",
       "      <th>16gb</th>\n",
       "      <th>16gbs</th>\n",
       "      <th>18th</th>\n",
       "      <th>1am</th>\n",
       "      <th>1jazzyjeff</th>\n",
       "      <th>1st</th>\n",
       "      <th>1u</th>\n",
       "      <th>2000ad</th>\n",
       "      <th>...</th>\n",
       "      <th>yikes</th>\n",
       "      <th>yldthng</th>\n",
       "      <th>yo</th>\n",
       "      <th>yooo</th>\n",
       "      <th>youd</th>\n",
       "      <th>youre</th>\n",
       "      <th>youve</th>\n",
       "      <th>z10</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zippos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.558987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>844 rows × 2915 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        10min  13apple  16gb  16gbs  18th  1am  1jazzyjeff  1st   1u  2000ad  \\\n",
       "0    0.000000      0.0   0.0    0.0   0.0  0.0         0.0  0.0  0.0     0.0   \n",
       "1    0.000000      0.0   0.0    0.0   0.0  0.0         0.0  0.0  0.0     0.0   \n",
       "2    0.000000      0.0   0.0    0.0   0.0  0.0         0.0  0.0  0.0     0.0   \n",
       "3    0.000000      0.0   0.0    0.0   0.0  0.0         0.0  0.0  0.0     0.0   \n",
       "4    0.558987      0.0   0.0    0.0   0.0  0.0         0.0  0.0  0.0     0.0   \n",
       "..        ...      ...   ...    ...   ...  ...         ...  ...  ...     ...   \n",
       "839  0.000000      0.0   0.0    0.0   0.0  0.0         0.0  0.0  0.0     0.0   \n",
       "840  0.000000      0.0   0.0    0.0   0.0  0.0         0.0  0.0  0.0     0.0   \n",
       "841  0.000000      0.0   0.0    0.0   0.0  0.0         0.0  0.0  0.0     0.0   \n",
       "842  0.000000      0.0   0.0    0.0   0.0  0.0         0.0  0.0  0.0     0.0   \n",
       "843  0.000000      0.0   0.0    0.0   0.0  0.0         0.0  0.0  0.0     0.0   \n",
       "\n",
       "     ...  yikes  yldthng   yo  yooo  youd  youre  youve  z10  zimmerman  \\\n",
       "0    ...    0.0      0.0  0.0   0.0   0.0    0.0    0.0  0.0        0.0   \n",
       "1    ...    0.0      0.0  0.0   0.0   0.0    0.0    0.0  0.0        0.0   \n",
       "2    ...    0.0      0.0  0.0   0.0   0.0    0.0    0.0  0.0        0.0   \n",
       "3    ...    0.0      0.0  0.0   0.0   0.0    0.0    0.0  0.0        0.0   \n",
       "4    ...    0.0      0.0  0.0   0.0   0.0    0.0    0.0  0.0        0.0   \n",
       "..   ...    ...      ...  ...   ...   ...    ...    ...  ...        ...   \n",
       "839  ...    0.0      0.0  0.0   0.0   0.0    0.0    0.0  0.0        0.0   \n",
       "840  ...    0.0      0.0  0.0   0.0   0.0    0.0    0.0  0.0        0.0   \n",
       "841  ...    0.0      0.0  0.0   0.0   0.0    0.0    0.0  0.0        0.0   \n",
       "842  ...    0.0      0.0  0.0   0.0   0.0    0.0    0.0  0.0        0.0   \n",
       "843  ...    0.0      0.0  0.0   0.0   0.0    0.0    0.0  0.0        0.0   \n",
       "\n",
       "     zippos  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  \n",
       "..      ...  \n",
       "839     0.0  \n",
       "840     0.0  \n",
       "841     0.0  \n",
       "842     0.0  \n",
       "843     0.0  \n",
       "\n",
       "[844 rows x 2915 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(tweets.Tweet)\n",
    "\n",
    "names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "tfidf_vectors = tfidf_vectors.toarray()\n",
    "tfidf_vectors = pd.DataFrame(tfidf_vectors, columns=names)\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae63ed9",
   "metadata": {},
   "source": [
    "#### 2. Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cdf3517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using count vectoroizer to create a document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(tweets.Tweet).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d97faed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document-Term Matrix\n",
    "DTM = pd.DataFrame(X, columns=cv.get_feature_names())\n",
    "\n",
    "#Remove terms that is conatined in less than 1% of the documents\n",
    "for col in list(DTM):\n",
    "    prop = DTM[col].sum()/DTM.shape[0]\n",
    "    if prop*100 < 1:\n",
    "        DTM = DTM.drop([col], axis=1)\n",
    "        \n",
    "selected_variables = list(DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "19984b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(844, 140)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e0712",
   "metadata": {},
   "source": [
    "#### 3. Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e26f171b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5c</th>\n",
       "      <th>5s</th>\n",
       "      <th>already</th>\n",
       "      <th>amazon</th>\n",
       "      <th>android</th>\n",
       "      <th>anyone</th>\n",
       "      <th>app</th>\n",
       "      <th>apples</th>\n",
       "      <th>apps</th>\n",
       "      <th>back</th>\n",
       "      <th>...</th>\n",
       "      <th>well</th>\n",
       "      <th>wont</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>wow</th>\n",
       "      <th>wtf</th>\n",
       "      <th>yall</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.207132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.240927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      5c        5s  already  amazon  android  anyone  app  apples  apps  back  \\\n",
       "69   0.0  0.000000      0.0     0.0      0.0     0.0  0.0     0.0   0.0   0.0   \n",
       "574  0.0  0.207132      0.0     0.0      0.0     0.0  0.0     0.0   0.0   0.0   \n",
       "165  0.0  0.000000      0.0     0.0      0.0     0.0  0.0     0.0   0.0   0.0   \n",
       "599  0.0  0.000000      0.0     0.0      0.0     0.0  0.0     0.0   0.0   0.0   \n",
       "696  0.0  0.000000      0.0     0.0      0.0     0.0  0.0     0.0   0.0   0.0   \n",
       "\n",
       "     ...  well  wont  work  would  wow  wtf  yall      year  yet  youre  \n",
       "69   ...   0.0   0.0   0.0    0.0  0.0  0.0   0.0  0.000000  0.0    0.0  \n",
       "574  ...   0.0   0.0   0.0    0.0  0.0  0.0   0.0  0.000000  0.0    0.0  \n",
       "165  ...   0.0   0.0   0.0    0.0  0.0  0.0   0.0  0.000000  0.0    0.0  \n",
       "599  ...   0.0   0.0   0.0    0.0  0.0  0.0   0.0  0.000000  0.0    0.0  \n",
       "696  ...   0.0   0.0   0.0    0.0  0.0  0.0   0.0  0.240927  0.0    0.0  \n",
       "\n",
       "[5 rows x 140 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the target variable\n",
    "y = tweets.Avg <= 0\n",
    "tfidf_vectors = tfidf_vectors[selected_variables]\n",
    "\n",
    "#For test-train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vectors, y, test_size = 0.2, stratify=y)\n",
    "\n",
    "#Train-Validation Split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.25, stratify=y_train)\n",
    "\n",
    "X_valid.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6853d41",
   "metadata": {},
   "source": [
    "### Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956869b7",
   "metadata": {},
   "source": [
    "#### 2. Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "39afbb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Tree Model Fitting\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "dt.fit(X_train,y_train)\n",
    "pred = dt.predict(X_valid)\n",
    "\n",
    "pred = dt.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9bc8f80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9,  52],\n",
       "       [  4, 104]], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix \n",
    "cm = confusion_matrix(y_valid,pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f1654aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score of Decision Tree with Depth=5: 0.6686390532544378\n",
      "F1_score of Decision Tree: 0.7878787878787878\n"
     ]
    }
   ],
   "source": [
    "# accuracy score of Decision tree\n",
    "print(\"Accuracy_score of Decision Tree with Depth=5:\",accuracy_score(y_valid,pred)) \n",
    "# f1 score of Decision tree\n",
    "print(\"F1_score of Decision Tree:\",f1_score(y_valid,pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c577f",
   "metadata": {},
   "source": [
    "## Fully Growned Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f1373cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train,y_train)\n",
    "pred = dt.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3d52f750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27, 34],\n",
       "       [37, 71]], dtype=int64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix \n",
    "cm = confusion_matrix(y_valid,pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "58119e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score of fully growned Decision Tree: 0.5798816568047337\n",
      "F1_score of Decision Tree: 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy_score of fully growned Decision Tree:\",accuracy_score(y_valid,pred)) # accuracy score of fully growned Decision tree\n",
    "print(\"F1_score of Decision Tree:\",f1_score(y_valid,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc817a",
   "metadata": {},
   "source": [
    "## Pruned Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2ab374f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00058955, 0.0007017 , 0.00081044, 0.00098814,\n",
       "       0.00118886, 0.00141543, 0.0017567 , 0.00177866, 0.00180443,\n",
       "       0.00190309, 0.00197628, 0.00251923, 0.0025552 , 0.00258769,\n",
       "       0.00263505, 0.00263505, 0.00270665, 0.00282326, 0.00296443,\n",
       "       0.00296443, 0.00296443, 0.00296443, 0.00316206, 0.00316206,\n",
       "       0.00316206, 0.00316206, 0.00329381, 0.00338792, 0.00341244,\n",
       "       0.00355731, 0.00361378, 0.00363775, 0.00368906, 0.00372678,\n",
       "       0.00375494, 0.00402053, 0.00441587, 0.0058182 , 0.00691872,\n",
       "       0.00747362, 0.00813023, 0.01073634, 0.01113556, 0.01218616])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pruning-Finding alpha\n",
    "path = dt.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "ccp_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bb21d5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0037549407114624497"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dictionary to store alpha and mae\n",
    "train_accuracy = {}         \n",
    "validation_accuracy = {}\n",
    "\n",
    "for i in ccp_alphas:\n",
    "    dt = DecisionTreeClassifier(ccp_alpha=i)\n",
    "    dt.fit(X_train,y_train)\n",
    "    pred_valid = dt.predict(X_valid)\n",
    "    pred_train = dt.predict(X_train)\n",
    "\n",
    "    train_accuracy[i] = accuracy_score(y_train,pred_train)\n",
    "    validation_accuracy[i] = accuracy_score(y_valid,pred_valid)\n",
    "\n",
    "#Optimum Cost Complexity Parameter    \n",
    "opt_ccp = max(validation_accuracy,key = validation_accuracy.get) \n",
    "#optimal CCP_alphas \n",
    "opt_ccp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f5041f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting pruned decision tree model using optimum alpha\n",
    "prun_dt = DecisionTreeClassifier(ccp_alpha=opt_ccp) \n",
    "prun_dt.fit(X_train,y_train)\n",
    "\n",
    "pred = prun_dt.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "13f8edb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score of Pruned Decision Tree: 0.6804733727810651\n",
      "F1_score of Decision Tree: 0.7804878048780488\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy_score of Pruned Decision Tree:\",accuracy_score(y_valid,pred)) # accuracy score of fully growned Decision tree\n",
    "print(\"F1_score of Decision Tree:\",f1_score(y_valid,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dca0632",
   "metadata": {},
   "source": [
    "## Bagged Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "be25d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree model as base classifier for bagging -ensemble\n",
    "dt = DecisionTreeClassifier()      \n",
    "\n",
    "#bagging with DT models\n",
    "baggedModel = ensemble.BaggingClassifier(base_estimator=dt) \n",
    "\n",
    "#fitting training data in bagged model\n",
    "baggedModel.fit(X_train,y_train) \n",
    "pred_bg = baggedModel.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a943bbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33, 28],\n",
       "       [41, 67]], dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix \n",
    "cm = confusion_matrix(y_valid,pred_bg)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c25b9faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score of Bagged Decision Tree: 0.591715976331361\n",
      "F1_score of Decision Tree: 0.7804878048780488\n"
     ]
    }
   ],
   "source": [
    "#accuracy of bagged decision tree model\n",
    "print(\"Accuracy_score of Bagged Decision Tree:\",accuracy_score(y_valid,pred_bg)) \n",
    "print(\"F1_score of Decision Tree:\",f1_score(y_valid,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f06fd",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "47015404",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "# fitting training data in random forest model\n",
    "rf.fit(X_train, y_train)        \n",
    "\n",
    "pred_rf= rf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ba136fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31, 30],\n",
       "       [38, 70]], dtype=int64)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix \n",
    "cm = confusion_matrix(y_valid,pred_rf)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "71d2bbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score of Random Forest: 0.5976331360946746\n",
      "F1_score of Decision Tree: 0.7804878048780488\n"
     ]
    }
   ],
   "source": [
    "#accuracy of RandomForest model\n",
    "print(\"Accuracy_score of Random Forest:\",accuracy_score(y_valid,pred_rf)) \n",
    "print(\"F1_score of Decision Tree:\",f1_score(y_valid,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2280084f",
   "metadata": {},
   "source": [
    "## Adaboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "08a183c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_boost = AdaBoostClassifier()\n",
    "# fitting training data in adaboost model\n",
    "ada_boost.fit(X_train, y_train) \n",
    "\n",
    "pred_ada = ada_boost.predict(X_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5d429afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22, 39],\n",
       "       [17, 91]], dtype=int64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix \n",
    "cm = confusion_matrix(y_valid,pred_ada)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0e963d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score of Adaboost: 0.6686390532544378\n",
      "F1_score of Decision Tree: 0.7804878048780488\n"
     ]
    }
   ],
   "source": [
    "#accuracy of adaboost model\n",
    "print(\"Accuracy_score of Adaboost:\",accuracy_score(y_valid,pred_ada)) \n",
    "print(\"F1_score of Decision Tree:\",f1_score(y_valid,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990fdab",
   "metadata": {},
   "source": [
    "# TF-IDF Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4f7c2499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decision tree with depth =5</th>\n",
       "      <th>Fully Growned DT</th>\n",
       "      <th>Pruned DT</th>\n",
       "      <th>Bagged DT</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Adaboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Decision tree with depth =5  Fully Growned DT  Pruned DT  Bagged DT  \\\n",
       "Accuracy                         0.64              0.61       0.64       0.65   \n",
       "F1_score                         0.76              0.70       0.76       0.76   \n",
       "\n",
       "          Random Forest  Adaboost  \n",
       "Accuracy           0.65      0.64  \n",
       "F1_score           0.70      0.70  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Decision tree with depth =5': [0.64,0.76], 'Fully Growned DT': [0.61,0.70], 'Pruned DT': [0.64,0.76], 'Bagged DT': [0.65,0.76], 'Random Forest': [0.65,0.70], 'Adaboost': [0.64,0.70] }, index=['Accuracy','F1_score'])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
